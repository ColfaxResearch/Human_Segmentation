{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Starting: Conda environment creation:\n",
    "- Before starting testing the performance, we need to prepare the appropriate conda environments:\n",
    "  \n",
    "  1- tf_PyPI: conda environment with standard Python distribution 3.6 and standard non Intel optimized Tensorflow 1.7.0.\n",
    "  \n",
    "  2- tf_intel: conda environment with Intel Python distribution 3.6 and Intel optimized Tensorflow 1.4.0.\n",
    "  \n",
    "- for more information about the environment creation, check \n",
    "[Conda_env_creation.ipynb](Conda_env_creation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Conda_env_creation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have 2 conda environments ready, we will start with tf_PyPI first\n",
    "- Note: Steps to switch kernels in jupyter notebook:\n",
    "\n",
    "    - From the toolbar on top, choose Kernel -> Change Kernel -> choose the kernel you want to switch to.\n",
    "    - Restart kernal from Kernel-> Restart\n",
    "\n",
    "<img src=\"notebook_data/dropdown.png\" style='border:2px solid gray'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: \n",
    "### Train the human segmentation model inside the conda environment tf_PyPI:\n",
    "- Standard python distribution\n",
    "- Standard Tensorflow 1.7.0\n",
    "- Read input data directly from desk and preprocess data on the fly, using tf.data API\n",
    "- Batch size = 32\n",
    "- Data format = NHWC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def parser_raw(...):\n",
    "    ...\n",
    "    ...\n",
    "    for i in range(self.batch_size):\n",
    "      #Preparing and preprocessing image\n",
    "      image1=cv2.imread(os.path.join(...))\n",
    "      image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "      image1 = cv2.resize(image1, (48, 48), ...)\n",
    "      image1 = np.reshape(image1, (48, 48, 3)).astype(np.float32)\n",
    "      image1 = np.divide(image1, 255.0)\n",
    "\n",
    "      mask1 = cv2.imread(os.path.join(...))\n",
    "      mask1 = cv2.resize(mask1, (48, 48), ...)\n",
    "      mask1 = cv2.cvtColor(mask1, cv2.COLOR_RGB2GRAY)\n",
    "      mask1 = np.reshape(mask1, (48, 48)).astype(np.float32)\n",
    "      mask1 = np.divide(mask1, 255.0)\n",
    "      yield image1, mask1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile case1.py\n",
    "#Case(1): standard Python distribution + standard tensorflow + raw_data\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job1\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_PyPI\n",
    "mkdir results\n",
    "python case1.py &> results/log1.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to set-up VTune and start profiling your application\n",
    "\n",
    "<img src=\"notebook_data/1-welcome.png\" style='border:2px solid gray'>\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "<img src=\"notebook_data/2-createproject.png\" style='border:2px solid gray'>\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "<img src=\"notebook_data/3-config-1.png\" style='border:2px solid gray'>\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "<img src=\"notebook_data/4-config-2.png\" style='border:2px solid gray'>\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "### To get more details about how to get VTune Amplifier and how to start using it visit this link:\n",
    "[Intel VTune Amplifier](https://software.intel.com/en-us/intel-vtune-amplifier-xe)\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results/log1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 180.5 sec\n",
    "- Training time per Epoch: 121.6 sec\n",
    "- Training time per Epoch: 125.2 sec\n",
    "\n",
    "## Observation 1:\n",
    "![Vtune-amplifier results, Bottom-up](notebook_data/case1-bottom-up.png)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "### Train our network inside the same conda environment tf_PyPI and apply data serialization:\n",
    "- Reading data directly from tfrecord using tf.data API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def parser_tf(..):\n",
    "\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                features={...})\n",
    "    _mask = features['train/mask']\n",
    "    image = features['train/image']\n",
    "\n",
    "    images_decoded = tf.decode_raw(image, tf.float32)\n",
    "    images_reshaped = tf.reshape(images_decoded, [48, 48, 3])\n",
    "\n",
    "    masks_decoded = tf.decode_raw(_mask, tf.float32)\n",
    "    masks_reshaped = tf.reshape(masks_decoded, [48, 48])\n",
    "    return images_reshaped, masks_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case2.py\n",
    "#Case(2): standard Python distribution + standard tensorflow + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job2\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_PyPI\n",
    "python case2.py &> results/log2.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results/log2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 35.5 sec\n",
    "- Training time per Epoch: 35.2 sec\n",
    "- Training time per Epoch: 35 sec\n",
    "\n",
    "\n",
    "## Observation 2:\n",
    "### VTune Bottom-up\n",
    "<kdb>![Vtune-amplifier results, Bottom-up](notebook_data/case2-bottom-up.png)</kdb>\n",
    "--------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APS report\n",
    "- Low floating-point unit utilization\n",
    "- 99% of the floating point instructions are 256-bit vector instructions\n",
    "\n",
    "[![Case(2) APS report](notebook_data/aps-case2.png)](notebook_data/aps-case2.html)\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Instructions\n",
    "\n",
    "![Array Vectorization](notebook_data/array-vectorization.png)\n",
    "---------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of Vector Instructions\n",
    "\n",
    "![Array Vectorization](notebook_data/intel-isa-evolution.png)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "### Now change the conda environment to tf_intel in order to start using Intel optimizations for Python and Tensorflow:\n",
    "- Intel distribution of Python 3.6\n",
    "- Intel optimized Tensorflow 1.4.0\n",
    "- Reading data from tfrecord file\n",
    "- Data_format = NHWC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case3.py\n",
    "#Case(3): Intel Python distribution + Intel optimized Tensorflow + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job3\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "python case3.py &> results/log3.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results/log3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 56.3 sec\n",
    "- Training time per Epoch: 55.2 sec\n",
    "- Training time per Epoch: 55.4 sec\n",
    "\n",
    "--------------\n",
    "\n",
    "## Observation 3:\n",
    "\n",
    "- Now 96.5% of the floating-point instructions are 512-bit vector instructions, but the total FPU is still low\n",
    "- Sever OpenMP Imbalance 71%\n",
    "- Number of OpenMP threads are 288\n",
    "    \n",
    "\n",
    "[![Case(3) APS report](notebook_data/aps-case3.png)](notebook_data/aps-case3.html)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-op/Intra-op Parallelism\n",
    "\n",
    "![Inter/Intra](notebook_data/inter-intra.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "### Now start tuning environment variables based on the guidance of the previous APS report:\n",
    "- To resolve the high OpenMP imbalance, set the Inter-op-threads=2 (equals to number of sockets) and the Intra-op-threads=12 (equals to number of physical cores, 6 cores/socket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case4.py\n",
    "#Case(4): Intel Python distribution + Intel optimized Tensorflow whl + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord', inter=2, intra=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile job4\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "python case4.py &> results/log4.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results/log4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 19.6 sec\n",
    "- Training time per Epoch: 18.2 sec\n",
    "- Training time per Epoch: 18.5 sec\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "## Observation 4:\n",
    "\n",
    "- OpenMP imbalance is now better, but we still can improve it more\n",
    "\n",
    "\n",
    "[![Case(4) APS report](notebook_data/aps-case4.png)](notebook_data/aps-case4.html)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread Affinity\n",
    "\n",
    "Ability to bind OpenMP threads to physical cores\n",
    "\n",
    "![Affinity](notebook_data/affinity-compact.png)\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "### More tuning for environment variables to reduce the openMP high imbalance:\n",
    "- Set KMP_AFFINITY=compact,1,granularity=fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case5.py\n",
    "#Case(4): Intel Python distribution + Intel optimized Tensorflow whl + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord', inter=2, intra=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job5\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "export KMP_AFFINITY=compact,1,granularity=fine\n",
    "python case5.py &> results/log5.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results/log5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 17 sec\n",
    "- Training time per Epoch: 16.51 sec\n",
    "- Training time per Epoch: 16.3 sec\n",
    "\n",
    "---\n",
    "\n",
    "## Observation 5:\n",
    "\n",
    "- Setting Affinity significantly reduces the openMP imbalance to below the threshold value (10%), but we still have high serial time\n",
    "- Also FPU increases to be 13 %, but it could be better\n",
    "\n",
    "\n",
    "[![Case(5) APS report](notebook_data/aps-case5.png)](notebook_data/aps-case5.html)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMP_BLOCKTIME\n",
    "\n",
    "Time the thread should wait after execution, before sleeping\n",
    "\n",
    "![BlockTime](notebook_data/kmp_blocktime.svg.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: \n",
    "### Setting KMP_BLOCKTIME to resolve high serial time and low FPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case6.py\n",
    "#Case(4): Intel Python distribution + Intel optimized Tensorflow whl + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord', inter=2, intra=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job6\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "export KMP_AFFINITY=compact,1,granularity=fine\n",
    "export KMP_BLOCKTIME=2\n",
    "python case6.py &> results/log6.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results/log6.txt\n",
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 14 sec\n",
    "- Training time per Epoch: 13.75 sec\n",
    "- Training time per Epoch: 13.74 sec\n",
    "\n",
    "-----\n",
    "\n",
    "## Observation 6:\n",
    "- Now the FPU is 2x better (31%)\n",
    "- Also the serial time is now less than 10% of the total elapsed time (8.75%)\n",
    "\n",
    "\n",
    "[![Case(6) APS report](notebook_data/aps-case6.png)](notebook_data/aps-case6.html)\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- The following chart is showing the performance improvement for all previous cases\n",
    "\n",
    "![All cases](notebook_data/chart-1-c009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the final result\n",
    "![All cases](notebook_data/chart-2-c009.png)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Tensorflow Optimizations for Intel architecture\n",
    "\n",
    "[TensorFlow* Optimizations for the Intel® Xeon® Scalable Processor](https://ai.intel.com/tensorflow-optimizations-intel-xeon-scalable-processor/#_ftn3)\n",
    "\n",
    "\n",
    "[Intel Optimized TensorFlow* Installation Guide](https://software.intel.com/articles/intel-optimized-tensorflow-installation-guide)\n",
    "\n",
    "\n",
    "[TensorFlow* Optimizations on Modern Intel® Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture)\n",
    "\n",
    "[TensorFlow Performance Guide](https://www.tensorflow.org/performance/performance_guide)\n",
    "\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Segmentation After Optimization\n",
    "Now, after using VTune Amplifier and APS guidance to optimize our model, human segmentation model is trained for 1000 epochs and ready for inference.\n",
    "\n",
    "To test the trained model using the validation dataset, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import visualize\n",
    "visualize(\"validate_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displayed images are:\n",
    "- Top left: original image\n",
    "- Top right: original image resized (48, 48, 3)\n",
    "- Bottom left: model output (48, 48)\n",
    "- Bottom right: ground truth resized (48, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own image\n",
    "Why don't you try your own image now?\n",
    "All what it needs is a person's image, it would provide better results if the image is of the following specs:\n",
    "\n",
    "- Person standing with full length in the mid of the image\n",
    "- Single person in the image\n",
    "- Contrast exists between the background and the person in the image\n",
    "\n",
    "Check the following image to have some idea about how should the image looks like:\n",
    "\n",
    "<img src=\"notebook_data/example-image-2.jpg\" width=\"200\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to try the human segmentation model by yourself:\n",
    "\n",
    "- Upload an image with the previous specs\n",
    "- place it in the same directory as the notebook \n",
    "- rename the image to \"input-image.jpg\"\n",
    "- Run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import visualize\n",
    "visualize(\"uploaded_image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
