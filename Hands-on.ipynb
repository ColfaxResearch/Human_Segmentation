{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Intel Software Development Tools For Maximizing Deep Learning Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Obtaining the Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use a traditional Python environment. At the beginning, you ran the script \"devcon18\", which installed a Conda environment with standard (PyPI) distribution of Python 3.6 and TensorFlow. If you wanted to reproduce this environment on your own system, you would run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda create -y -n tf_PyPI python=3.6 -c anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installation is done, you should be able to use the tools in it by activating them. Let's check if this works. Run the cell below (highlight it and press Ctrl+Enter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the environment \"tf_PyPI\" in the list, you are good to go. If you don't see it, you may need to wait a couple of minutes for the installation to finish in the background. Rerun the above cell in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train the human segmentation model inside the conda environment tf_PyPI:\n",
    "- Standard python distribution\n",
    "- Standard Tensorflow 1.7.0\n",
    "- Read input data directly from desk and preprocess data on the fly, using tf.data API\n",
    "- Batch size = 32\n",
    "- Data format = NHWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a snippet of the code in our application that reads and prepares the data that is fed into the training pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def parser_raw(...):\n",
    "    ...\n",
    "    ...\n",
    "    for i in range(self.batch_size):\n",
    "      #Preparing and preprocessing image\n",
    "      image1=cv2.imread(os.path.join(...))\n",
    "      image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "      image1 = cv2.resize(image1, (48, 48), ...)\n",
    "      image1 = np.reshape(image1, (48, 48, 3)).astype(np.float32)\n",
    "      image1 = np.divide(image1, 255.0)\n",
    "\n",
    "      mask1 = cv2.imread(os.path.join(...))\n",
    "      mask1 = cv2.resize(mask1, (48, 48), ...)\n",
    "      mask1 = cv2.cvtColor(mask1, cv2.COLOR_RGB2GRAY)\n",
    "      mask1 = np.reshape(mask1, (48, 48)).astype(np.float32)\n",
    "      mask1 = np.divide(mask1, 255.0)\n",
    "      yield image1, mask1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a training script case1.py. It will train for 3 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile case1.py\n",
    "#Case(1): standard Python distribution + standard tensorflow + raw_data\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='raw_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below that creates the job script. This script will later go into the cloud queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job1\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_PyPI\n",
    "mkdir results\n",
    "python case1.py > results/log1.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the job to the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job1 > job1-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you wait:\n",
    "\n",
    "### How to set up VTune and start profiling your application\n",
    "\n",
    "We do not provide VTune on the DevCloud, but the instructor will explain the workflow of setting up a VTune project.\n",
    "\n",
    "<img src=\"notebook_data/1-welcome.png\" style='border:2px solid gray'>\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "<img src=\"notebook_data/2-createproject.png\" style='border:2px solid gray'>\n",
    "\n",
    "----------------------------------------------------\n",
    "\n",
    "<img src=\"notebook_data/3-config-1.png\" style='border:2px solid gray'>\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "<img src=\"notebook_data/4-config-2.png\" style='border:2px solid gray'>\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "### To get more details about how to get VTune Amplifier and how to start using it visit this link:\n",
    "[Intel VTune Amplifier](https://software.intel.com/en-us/intel-vtune-amplifier-xe)\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, our job has finished. How fast is the application running? To find out, check the log by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./jobresult `cat job1-id` results/log1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 180.5 sec\n",
    "- Training time per Epoch: 121.6 sec\n",
    "- Training time per Epoch: 125.2 sec\n",
    "\n",
    "Is it good? What is the system busy with during training? To find out, we analyzed the training in VTune and the results are shown below.\n",
    "\n",
    "### Observation 1:\n",
    "\n",
    "According to VTune, an OpenCV module uses around 12% of the CPU time. This is suspicious: why would OpenCV be used during training?\n",
    "![Vtune-amplifier results, Bottom-up](notebook_data/case1-bottom-up.png)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Improve Data Input Pipeline\n",
    "### Apply data serialization:\n",
    "- To get rid of the OpenCV overhead, we will use a recommended TensorFlow technique\n",
    "- Reading data directly from tfrecord using tf.data API\n",
    "- The snippet below illustrates the new approach."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def parser_tf(..):\n",
    "\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                features={...})\n",
    "    _mask = features['train/mask']\n",
    "    image = features['train/image']\n",
    "\n",
    "    images_decoded = tf.decode_raw(image, tf.float32)\n",
    "    images_reshaped = tf.reshape(images_decoded, [48, 48, 3])\n",
    "\n",
    "    masks_decoded = tf.decode_raw(_mask, tf.float32)\n",
    "    masks_reshaped = tf.reshape(masks_decoded, [48, 48])\n",
    "    return images_reshaped, masks_reshaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create the training application file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case2.py\n",
    "#Case(2): standard Python distribution + standard tensorflow + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to create the new job script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job2\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_PyPI\n",
    "python case2.py > results/log2.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, submit the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job2 > job2-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to wait a few minutes for the job to finish. When it's done, view the results by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./jobresult `cat job2-id` results/log2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 35.5 sec\n",
    "- Training time per Epoch: 35.2 sec\n",
    "- Training time per Epoch: 35 sec\n",
    "\n",
    "\n",
    "### Observation 2:\n",
    "#### VTune Bottom-up\n",
    "<kdb>![Vtune-amplifier results, Bottom-up](notebook_data/case2-bottom-up.png)</kdb>\n",
    "--------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked. OpenCV is no longer one of the top hotspots, and the time per epoch went down from 120 to 35 seconds — a lot!\n",
    "\n",
    "So, is this a good performance for our application? We will find out. This time we will do it using the Intel software development tool Application Performance Snapshot. Compared to VTune, it gives you less information, but APS is much easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APS report\n",
    "Again, the Intel DevCloud does not support APS, so we collected performance data on a different system. To run the analysis with APS, we just had to prepend the Python invocation command with \"aps\":"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "aps python case2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is shown below.\n",
    "\n",
    "[![Case(2) APS report](notebook_data/aps-case2.png)](notebook_data/aps-case2.html)\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "---\n",
    "\n",
    "What is the most striking aspect of this report?\n",
    "- Low floating-point unit utilization\n",
    "- 99% of the floating point instructions are 256-bit vector instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Instructions\n",
    "\n",
    "Intel architecture processors have not only multiple cores, but also parallelism inside each core in the form of vector instructions. They allow you to apply a stream of mathematical instructions to not one, but up to 16 floating-point numbers at once. \n",
    "\n",
    "![Array Vectorization](notebook_data/array-vectorization.png)\n",
    "---------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of Vector Instructions\n",
    "\n",
    "![Array Vectorization](notebook_data/intel-isa-evolution.png)\n",
    "\n",
    "-----\n",
    "\n",
    "Processors on the Intel AI DevCloud are Intel Xeon Gold 6128 CPUs supporting 512-bit AVX-512 vector instructions. See for yourself: run the cell below. So why are we using 256-bit instructions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use Intel-Optimized TensorFlow\n",
    "\n",
    "The reason why our code uses 256-bit instructions is that our framework, TensorFlow, is not compiled to use AVX-512. Let's get Intel-optimized TensorFlow compiled with the Intel Math Kernel Library with AVX-512 support for DNN promitives.\n",
    "\n",
    "You already installed it when you ran \"devcon18\" from the terminal. If you want to do it on your system later, you will have to execute the following commands."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda config --add channels intel\n",
    "conda create -y -n tf_intel intelpython3_core python=3.6\n",
    "source activate tf_intel\n",
    "pip install https://anaconda.org/intel/tensorflow/1.6.0/download/tensorflow-1.6.0-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now change the conda environment to tf_intel to start using Intel optimizations for Python and Tensorflow:\n",
    "- Intel distribution of Python 3.6\n",
    "- Intel optimized Tensorflow 1.4.0\n",
    "- Reading data from tfrecord file\n",
    "- Data_format = NHWC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case3.py\n",
    "#Case(3): Intel Python distribution + Intel optimized Tensorflow + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the job script, this time using the Conda environment \"tf_intel\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job3\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "python case3.py > results/log3.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the job to the queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job3 > job3-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few minutes later, check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./jobresult `cat job3-id` results/log3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 56.3 sec\n",
    "- Training time per Epoch: 55.2 sec\n",
    "- Training time per Epoch: 55.4 sec\n",
    "\n",
    "--------------\n",
    "\n",
    "### Observation 3:\n",
    "\n",
    "Oops! The training time increased from 35 to 55 seconds. What does that mean? We ran the above job through APS and found the following.\n",
    "\n",
    "- Now 96.5% of the floating-point instructions are 512-bit vector instructions, which is what we wanted\n",
    "- However, the total FPU utilization is still low\n",
    "- Sever OpenMP Imbalance 71%\n",
    "- Number of OpenMP threads is 288 - that's way too many for our 12-core server!\n",
    "    \n",
    "\n",
    "[![Case(3) APS report](notebook_data/aps-case3.png)](notebook_data/aps-case3.html)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inter-op/Intra-op Parallelism\n",
    "\n",
    "As APS report shows, out-of-the-box run with Intel-optimized tensorflow uses too many OpenMP threads. That is because the default parallel strategy is not optimal for this CPU. Fortunately, TensorFlow allows parallel strategy tuning by with inter_op/intra_op threads parameter.\n",
    "\n",
    "![Inter/Intra](notebook_data/inter-intra.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "### Now start tuning environment variables based on the guidance of the previous APS report:\n",
    "- To resolve the high OpenMP imbalance, set the Inter-op-threads=2 (equals to number of sockets) and the Intra-op-threads=12 (equals to number of physical cores, 6 cores/socket)\n",
    "- Repeat the job submission process with this new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case4.py\n",
    "#Case(4): Intel Python distribution + Intel optimized Tensorflow whl + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord', inter=2, intra=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile job4\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "python case4.py > results/log4.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job4 > job4-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few minutes later, check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./jobresult `cat job4-id` results/log4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time per Epoch: 19.6 sec\n",
    "- Training time per Epoch: 18.2 sec\n",
    "- Training time per Epoch: 18.5 sec\n",
    "\n",
    "This looks very good. The best we had with standard TensorFlow (256-bit instructions) is 35 seconds per epoch. With Intel-optimized TensorFlow, we use AVX-512, and after tuning the parallel strategy, it gives us a speedup by almost 2x, finishing each epoch in just over 18 seconds.\n",
    "\n",
    "----------\n",
    "\n",
    "Is that good? Are we done?\n",
    "\n",
    "\n",
    "### Observation 4:\n",
    "\n",
    "- We ran the code through APS again, and the results are shown below\n",
    "- OpenMP imbalance is now better, but we still can improve it more\n",
    "\n",
    "\n",
    "[![Case(4) APS report](notebook_data/aps-case4.png)](notebook_data/aps-case4.html)\n",
    "---------------------------------------------------------------------------------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread Affinity\n",
    "\n",
    "Because Intel-optimized TensorFlow uses MKL, its threading is implemented with OpenMP. You can often reduce OpenMP load imbalance by binding threads to physical cores (setting thread affinity).\n",
    "\n",
    "![Affinity](notebook_data/affinity-compact.png)\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "### More tuning for environment variables to reduce the openMP high imbalance\n",
    "- Set KMP_AFFINITY=compact,1,granularity=fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case5.py\n",
    "#Case(4): Intel Python distribution + Intel optimized Tensorflow whl + tfrecord\n",
    "from train_model import train_model\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord', inter=2, intra=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job5\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "export KMP_AFFINITY=compact,1,granularity=fine\n",
    "python case5.py > results/log5.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job5 > job5-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a minute, check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./jobresult `cat job5-id` results/log5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time Epoch 0: 17 sec\n",
    "- Training time Epoch 1: 16.51 sec\n",
    "- Training time Epoch 2: 16.3 sec\n",
    "\n",
    "From 18 seconds per epoch, the training time is now around 16 seconds, so it was an improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### Observation 5:\n",
    "\n",
    "- Setting Affinity significantly reduces the openMP imbalance to below the threshold value (10%), but we still have high serial time\n",
    "- Also FPU increases to be 13 %, but it could be better\n",
    "- We ran APS once more to see what might be the bottleneck of the application. The result is shown below.\n",
    "\n",
    "\n",
    "[![Case(5) APS report](notebook_data/aps-case5.png)](notebook_data/aps-case5.html)\n",
    "---------------------------------------------------------------------------------\n",
    "---\n",
    "\n",
    "Even though the metric \"Serial Time\" is not highlighted in red, it is quite high. One of the reasons that the serial time can be longer than needed is that the processor is busy with unnecessary tasks during the serial period. For example, it may be busy maintaining the machinery of OpenMP threads between parallel regions. Fortunately, OpenMP has a control knob for this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMP_BLOCKTIME\n",
    "\n",
    "This Intel OpenMP parameter controls the time the thread should wait after execution, before sleeping. The default is 200 milliseconds.\n",
    "\n",
    "![BlockTime](notebook_data/kmp_blocktime.svg.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: \n",
    "### Setting KMP_BLOCKTIME to resolve high serial time and low FPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the thread machinery overhead in serial regions, we will reduce the block time to 2 ms. Run the job below to see the impact of this optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile case6.py\n",
    "#Case(4): Intel Python distribution + Intel optimized Tensorflow whl + tfrecord\n",
    "from train_model import train_model\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "model = train_model()\n",
    "model.train(32, 3, subset='train', source='tfrecord', inter=2, intra=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job6\n",
    "cd $PBS_O_WORKDIR\n",
    "source activate tf_intel\n",
    "export KMP_AFFINITY=compact,1,granularity=fine\n",
    "export KMP_BLOCKTIME=2\n",
    "python case6.py > results/log6.txt\n",
    "\n",
    "# need an empty line at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit the job to the queue\n",
    "!qsub job6 > job6-id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a minute, check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./jobresult `cat job6-id` results/log6.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your output result should be close to the following:\n",
    "\n",
    "- Training time Epoch 0: 14 sec\n",
    "- Training time Epoch 1: 13.75 sec\n",
    "- Training time Epoch 2: 13.74 sec\n",
    "\n",
    "Another improvement: from 16 seconds per epoch to 14 seconds per epoch.\n",
    "\n",
    "-----\n",
    "\n",
    "### Observation 6:\n",
    "- Now the FPU is 2x better (31%)\n",
    "- Also the serial time is now less than 10% of the total elapsed time (8.75%)\n",
    "\n",
    "\n",
    "[![Case(6) APS report](notebook_data/aps-case6.png)](notebook_data/aps-case6.html)\n",
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- The following chart is showing the performance improvement for all previous cases\n",
    "\n",
    "![All cases](notebook_data/chart-1-c009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The chart below isolates the best performance obtained with standard TensorFlow (35 seconds per epoch) and the best timing with Intel-optimized TensorFlow (14 seconds per epoch).\n",
    "![All cases](notebook_data/chart-2-c009.png)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Tensorflow Optimizations for Intel architecture\n",
    "\n",
    "[TensorFlow* Optimizations for the Intel® Xeon® Scalable Processor](https://ai.intel.com/tensorflow-optimizations-intel-xeon-scalable-processor/#_ftn3)\n",
    "\n",
    "\n",
    "[Intel Optimized TensorFlow* Installation Guide](https://software.intel.com/articles/intel-optimized-tensorflow-installation-guide)\n",
    "\n",
    "\n",
    "[TensorFlow* Optimizations on Modern Intel® Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture)\n",
    "\n",
    "[TensorFlow Performance Guide](https://www.tensorflow.org/performance/performance_guide)\n",
    "\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Learned\n",
    "\n",
    "- Intel Distribution for Python delivers significant performance gain in deep learning over the standard Python distribution\n",
    "- Intel VTune Amplifier is useful for detecting the specific modules of the application that hold back the performance\n",
    "- Intel Application Performance Snapshot is useful for detecting general issues with the application\n",
    "\n",
    "Naturally, the preformance analysis tools and methods discussed here are not limited to deep learning or Python. They apply to C/C++/Fortran programming in other domains as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Segmentation After Optimization\n",
    "Now, after using VTune Amplifier and APS guidance to optimize our model, we ran the training process for 1000 epochs. It took 14000 seconds (4 hours). Compare it to the estimated training time without optimization, which would have been 36000 seconds (10 hours).\n",
    "\n",
    "We saved the trained weights, so you don't have to wait for 4 hours to see how the model works. Run the next cell to analyze a random image from the validation dataset and compare the prediction of the model to the ground truth mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import visualize\n",
    "visualize(\"validate_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displayed images are:\n",
    "- Top left: original image\n",
    "- Top right: original image resized (48, 48, 3)\n",
    "- Bottom left: model output (48, 48)\n",
    "- Bottom right: ground truth resized (48, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own image\n",
    "Why don't you try your own image now?\n",
    "All what it needs is a person's image, it would provide better results if the image is of the following specs:\n",
    "\n",
    "- Person standing with full length in the mid of the image\n",
    "- Single person in the image\n",
    "- Contrast exists between the background and the person in the image\n",
    "\n",
    "Check the following image to have some idea about how should the image looks like:\n",
    "\n",
    "<img src=\"notebook_data/example-image-2.jpg\" width=\"200\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to try the human segmentation model by yourself:\n",
    "\n",
    "- Upload an image with the previous specs:\n",
    "    - From the Jyputer home page, click on Upload button on the top right to upload your image to the DevCloud\n",
    "- Make sure to place it in the same directory as the notebook \n",
    "- Rename the image to \"input-image.jpg\"\n",
    "- Run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import visualize\n",
    "visualize(\"uploaded_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also try any image from the web:\n",
    "- Copy the image url to the url variable in the following cell, make sure that url is between the double quotes \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import visualize\n",
    "url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTcfMvfzFBzo_CSC3p0_0mcBNKAiD-4bbkrUlRSsR-mep1nE2yqxQ\"\n",
    "visualize(\"url\", url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
